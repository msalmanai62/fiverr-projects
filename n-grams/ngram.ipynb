{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def build_unigram_model(training_file):\n",
    "    word_counts = defaultdict(int)\n",
    "    total_words = 0\n",
    "    \n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Preprocess the line\n",
    "            line = preprocess_text(line)\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "                total_words += 1\n",
    "    \n",
    "    word_probs = {word: count/total_words for word, count in word_counts.items()}\n",
    "    return word_probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_file = 'train2.txt'\n",
    "# wrds = []\n",
    "# with open(training_file, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             # Preprocess the line\n",
    "#             line = preprocess_text(line)\n",
    "#             words = line.strip().split()\n",
    "#             wrds.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'train2.txt'\n",
    "word_probs=build_unigram_model(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def test_unigram_model(model, test_file):\n",
    "    # Process each line in the test file\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            sentence_prob = 1\n",
    "            for word in words:\n",
    "                # Use a small probability for unknown words\n",
    "                word_prob = model.get(word, 1e-6)\n",
    "                sentence_prob *= word_prob\n",
    "            \n",
    "            print(f\"Sentence: {line.strip()}\\nProbability: {math.log(sentence_prob)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: a\n",
      "Probability: -4.03731822540442\n",
      "\n",
      "Sentence: b c\n",
      "Probability: -24.91484902136226\n",
      "\n",
      "Sentence: a b c d\n",
      "Probability: -40.967796442038825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = word_probs\n",
    "test_file = 'test1.txt'\n",
    "test_unigram_model(model, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Wolf\n",
      "Probability: -13.815510557964274\n",
      "\n",
      "Sentence: In the jungle\n",
      "Probability: -24.64663467280577\n",
      "\n",
      "Sentence: Rustle in the grass .\n",
      "Probability: -33.0732457544786\n",
      "\n",
      "Sentence: What could go wrong ?\n",
      "Probability: -41.35452071759269\n",
      "\n",
      "Sentence: I swear I am not making this up .\n",
      "Probability: -76.1685325974044\n",
      "\n",
      "Sentence: But old Mr. Toad will leave one day .\n",
      "Probability: -77.37797250211233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = word_probs\n",
    "test_file = 'test2.txt'\n",
    "test_unigram_model(model, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Wolf\n",
      "Probability: -13.815510557964274\n",
      "\n",
      "Sentence: In the jungle\n",
      "Probability: -24.64663467280577\n",
      "\n",
      "Sentence: Rustle in the grass .\n",
      "Probability: -33.0732457544786\n",
      "\n",
      "Sentence: What could go wrong ?\n",
      "Probability: -41.35452071759269\n",
      "\n",
      "Sentence: I swear I am not making this up .\n",
      "Probability: -76.1685325974044\n",
      "\n",
      "Sentence: But old Mr. Toad will leave one day .\n",
      "Probability: -77.37797250211233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = word_probs\n",
    "test_file = 'test2.txt'\n",
    "test_unigram_model(model, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unigram_model(model, test_file):\n",
    "    # Process each line in the test file\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            sentence_prob = 1\n",
    "            for word in words:\n",
    "                # Use a small probability for unknown words\n",
    "                word_prob = model.get(word, 1e-6)\n",
    "                sentence_prob *= word_prob\n",
    "            \n",
    "            print(f\"Sentence: {line.strip()}\\nProbability: {sentence_prob}\\n\")\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python3 ngrams.py [training file] [test file]\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    training_file, test_file = sys.argv[1], sys.argv[2]\n",
    "    model = build_unigram_model(training_file)\n",
    "    test_unigram_model(model, test_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    words = training_text.split()\n",
    "    total_words = len(words)\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts, total_words\n",
    "\n",
    "def unigram_probability(sentence, word_counts, total_words):\n",
    "    words = preprocess_text(sentence).split()\n",
    "    log_prob = 0\n",
    "    for word in words:\n",
    "        word_prob = word_counts[word] / total_words\n",
    "        log_prob += math.log2(word_prob)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    sentences = training_text.split('\\n')\n",
    "    bigram_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = ['<s>'] + sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "            word_counts[words[i]] += 1\n",
    "            \n",
    "    return bigram_counts, word_counts\n",
    "\n",
    "\n",
    "def bigram_probability(sentence, bigram_counts, word_counts):\n",
    "    words = ['<s>'] + preprocess_text(sentence).split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        bigram_prob = bigram_counts[bigram] / word_counts[words[i]]\n",
    "        log_prob += math.log2(bigram_prob) if bigram_prob > 0 else float('-inf')\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_model(training_file):\n",
    "    # sentences = training_text.lower().split('\\n')\n",
    "\n",
    "    bigram_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Preprocess the line\n",
    "            # line = preprocess_text(line)\n",
    "            # words = line.strip().split()\n",
    "            words = ['<s>'] + preprocess_text(line).split()\n",
    "            for i in range(len(words) - 1):\n",
    "                bigram = (words[i], words[i+1])\n",
    "                bigram_counts[bigram] += 1\n",
    "                word_counts[words[i]] += 1        \n",
    "            \n",
    "    return bigram_counts, word_counts\n",
    "\n",
    "def bigram_probability(sentence, bigram_counts, word_counts):\n",
    "    words = ['<s>'] + preprocess_text(sentence).split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        bigram_prob = bigram_counts[bigram] / word_counts[words[i]]\n",
    "        log_prob += math.log2(bigram_prob) if bigram_prob > 0 else float('-inf')\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_add_one_smoothing(sentence, bigram_counts, word_counts, V):\n",
    "    words = ['<s>'] + preprocess_text(sentence).split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        bigram_prob = (bigram_counts[bigram] + 1) / (word_counts[words[i]] + V)\n",
    "        log_prob += math.log2(bigram_prob)\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 9, 2: 4, 8: 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([2,5,8,2,2,2,5,5,5,5,5,5,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# reading and preprocessing the text data file\n",
    "def preprocess_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "    return text\n",
    "\n",
    "# unigram model \n",
    "def build_unigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    words = training_text.split() # all words or unigrams in the text file\n",
    "    total_words = len(words) # total number of unigrams\n",
    "    word_counts = Counter(words) # make dictionary of words with counts\n",
    "    return word_counts, total_words\n",
    "\n",
    "# unigram model probability calculating function\n",
    "def unigram_probability(sentence, word_counts, total_words):\n",
    "    words = sentence.split() # split the sentance to words\n",
    "    log_prob = 0\n",
    "    for word in words:\n",
    "        word_prob = word_counts[word] / total_words if word in word_counts else 0 # calculating the probabilty of each word in sentence\n",
    "        log_prob += math.log2(word_prob) if word_prob > 0 else float('-inf') # adding the each words probability to total probability after applying log2\n",
    "    return log_prob if log_prob!=float('-inf') else \"undefined\"\n",
    "# bigram model\n",
    "def build_bigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    sentences = training_text.split('\\n')\n",
    "    bigram_counts = defaultdict(int) # initializing an empty dictionary for bigram counts\n",
    "    word_counts = defaultdict(int) # initializing an empty dictionary for words/unigram counts\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = ['<s>'] + sentence.split() # adding special character <s> in each\n",
    "        for i in range(len(words) - 1): # \n",
    "            bigram = (words[i], words[i+1])  # making bigrams -> (bigram = current_word + next_word)\n",
    "            bigram_counts[bigram] += 1 # bigram frequency calculation\n",
    "            word_counts[words[i]] += 1 # unigram frequency calculation\n",
    "            \n",
    "    return bigram_counts, word_counts\n",
    "\n",
    "# bigram probability calculation without smoothing \n",
    "def bigram_probability(sentence, bigram_counts, word_counts):\n",
    "    words = ['<s>'] + sentence.split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        bigram_prob = bigram_counts[bigram] / word_counts[words[i]] if bigram_counts[bigram] > 0 else 1\n",
    "        log_prob += math.log2(bigram_prob) if bigram_prob > 0 else float('-inf')\n",
    "    return log_prob if log_prob!=float('-inf') else \"undefined\"\n",
    "\n",
    "# bigram probability calculation smoothing \n",
    "def bigram_add_one_smoothing(sentence, bigram_counts, word_counts, V): # V = len(unigram_counts_for_bigrams) + 1  # +1 for <s> token\n",
    "    words = ['<s>'] + sentence.split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        bigram_prob = (bigram_counts[bigram] + 1) / (word_counts[words[i]] + V)\n",
    "        log_prob += math.log2(bigram_prob)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probability(sentence, word_counts, total_words):\n",
    "    words = sentence.split()\n",
    "    log_prob = 0\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_prob = word_counts[word] / total_words\n",
    "        else:\n",
    "            word_prob = word_counts['<UNK>'] / total_words  # Use <UNK> token for unknown words\n",
    "        log_prob += math.log2(word_prob)\n",
    "    return log_prob\n",
    "\n",
    "def bigram_probability(sentence, bigram_counts, word_counts):\n",
    "    words = ['<s>'] + sentence.split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        if bigram in bigram_counts and words[i] in word_counts:\n",
    "            bigram_prob = bigram_counts[bigram] / word_counts[words[i]]\n",
    "        else:\n",
    "            bigram_prob = bigram_counts[('<UNK>', words[i+1])] / word_counts['<UNK>']  # Use <UNK> token for unknown bigrams or words\n",
    "        log_prob += math.log2(bigram_prob)\n",
    "    return log_prob\n",
    "\n",
    "def build_unigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    words = training_text.split()\n",
    "    total_words = len(words)\n",
    "    word_counts = Counter(words)\n",
    "    word_counts['<UNK>'] = 1  # Initialize count for <UNK> token to 1\n",
    "    return word_counts, total_words\n",
    "\n",
    "def build_bigram_model(training_file):\n",
    "    training_text = preprocess_text_file(training_file)\n",
    "    sentences = training_text.split('\\n')\n",
    "    bigram_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = ['<s>'] + sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "            word_counts[words[i]] += 1\n",
    "            if words[i+1] not in word_counts:  # Add unseen words to word_counts with count 1\n",
    "                word_counts[words[i+1]] = 1\n",
    "            else:\n",
    "                word_counts[words[i+1]] += 1\n",
    "    bigram_counts[('<UNK>', '<UNK>')] = 1  # Initialize count for unknown bigrams to 1\n",
    "    return bigram_counts, word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"This', 'is', 'a', 'funny-looking', 'sentence\",', 'she', 'said!']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('\"This is a funny-looking sentence\", she said!').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts, word_counts = build_bigram_model(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'<s>': 1, 'a': 1, 'b': 1, 'c': 1, 'd': 1})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('<s>', 'a'): 1,\n",
       "             ('a', 'b'): 1,\n",
       "             ('b', 'c'): 1,\n",
       "             ('c', 'd'): 1,\n",
       "             ('d', 'b'): 1})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models_with_file(training_file, test_file):\n",
    "    \"\"\"\n",
    "    Test the specified language model with sentences from a test file.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_file: Path to the test text file.\n",
    "    - training_file: Path to the training text file.\n",
    "    \"\"\"\n",
    "    # unigram model\n",
    "    word_counts, total_words = build_unigram_model(training_file)\n",
    "    # bigram model\n",
    "    bigram_counts, unigram_counts_for_bigrams = build_bigram_model(training_file)\n",
    "    V = len(unigram_counts_for_bigrams)-1  # +1 for <s> token\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as file:\n",
    "        sentences = file.readlines()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        preprocessed_sentence = sentence.lower().strip()\n",
    "\n",
    "        # unigram model probs\n",
    "        log_prob_uni = unigram_probability(preprocessed_sentence, word_counts, total_words)\n",
    "\n",
    "        # bigram probs\n",
    "        log_prob_bi = bigram_probability(preprocessed_sentence, bigram_counts, unigram_counts_for_bigrams)\n",
    "\n",
    "        # bigram probs with smoothing\n",
    "        log_prob_bi_smoth = bigram_add_one_smoothing(preprocessed_sentence, bigram_counts, unigram_counts_for_bigrams, V)\n",
    "        \n",
    "        print(f\"S = {sentence.strip()}\")\n",
    "        print(f\"Unsmoothed Unigrams, logprob(S) = {round(log_prob_uni, 4) if log_prob_uni!='undefined' else log_prob_uni}\")\n",
    "        print(f\"Unsmoothed Bigrams, logprob(S) = {round(log_prob_bi, 4) if log_prob_bi!='undefined' else log_prob_bi}\")\n",
    "        print(f\"Smoothed Bigrams, logprob(S) = {log_prob_bi_smoth:.4f}\")\n",
    "        print() # add empty line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S = a\n",
      "Unsmoothed Unigrams, logprob(S) = -2.3219\n",
      "Unsmoothed Bigrams, logprob(S) = 0.0\n",
      "Smoothed Bigrams, logprob(S) = -1.3219\n",
      "\n",
      "S = b c\n",
      "Unsmoothed Unigrams, logprob(S) = -3.6439\n",
      "Unsmoothed Bigrams, logprob(S) = 0.0\n",
      "Smoothed Bigrams, logprob(S) = -3.6439\n",
      "\n",
      "S = a b c d\n",
      "Unsmoothed Unigrams, logprob(S) = -8.2877\n",
      "Unsmoothed Bigrams, logprob(S) = 0.0\n",
      "Smoothed Bigrams, logprob(S) = -5.2877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_file = \"train1.txt\"\n",
    "test_file = \"test1.txt\"\n",
    "test_models_with_file(training_file, test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
