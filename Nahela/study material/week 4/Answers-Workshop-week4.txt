TASK 1a.
python BayesNetInference.py RejectionSampling ..\config\config-alarm.txt "P(B|J=true,M=true)" 1000
run 1: P(B)={'true': 0.0, 'false': 1.0}
run 2: P(B)={'true': 0.3333333333333333, 'false': 0.6666666666666666}
run 3: P(B)={'true': 0.25, 'false': 0.75}

python BayesNetInference.py RejectionSampling ..\config\config-alarm.txt “P(E|J=true,M=true)” 1000
run 1: P(E)={'true': 0.0, 'false': 1.0}
run 2: P(E)={'true': 0.25, 'false': 0.75}
run 3: P(E)={'true': 0.0, 'false': 1.0}

From the above, inconsistent results across runs can be noted.


TASK 1b.
ALARM network								
Run	N=100		N=1000		N=10000		N=100000	
	true	false	true	false	true	false	true	false
1	NA	NA	0.0000	1.0000	0.3636	0.6364	0.2172	0.7828
2	NA	NA	1.0000	0.0000	0.2381	0.7619	0.2865	0.7135
3	NA	NA	0.5000	0.5000	0.3200	0.6800	0.3095	0.6905
4	NA	NA	0.0000	1.0000	0.1429	0.8571	0.1762	0.8238
5	NA	NA	0.5000	0.5000	0.1500	0.8500	0.2525	0.7475
AVG	NA	NA	0.4000	0.6000	0.2429	0.7571	0.2484	0.7516
STD	NA	NA	0.4183	0.4183	0.0990	0.0990	0.0534	0.0534


TASK 1c.

python BayesNetInference.py InferenceByEnumeration ..\config\config-alarm.txt “P(B|J=true,M=true)”
normalised P(B)={'true': 0.2841718353643929, 'false': 0.7158281646356071}

python BayesNetInference.py InferenceByEnumeration ..\config\config-alarm.txt “P(E|J=true,M=true)”
normalised P(E)={'true': 0.17606683840507922, 'false': 0.8239331615949208}

python BayesNetInference.py RejectionSampling ..\config\config-alarm.txt “P(B|J=true,M=true)” 100000
P(B)={'true': 0.2878787878787879, 'false': 0.7121212121212122}

python BayesNetInference.py RejectionSampling ..\config\config-alarm.txt “P(E|J=true,M=true)” 100000
P(E)={'true': 0.1642512077294686, 'false': 0.8357487922705314}

It can be noted that Rejection Sampling produces similar results as exact inference using 100K samples.



TASK 2a.
python BayesNetInference.py RejectionSampling ..\config\config-sprinkler.txt "P(S|C=true)" 1000
P(S)={'true': 0.08704453441295547, 'false': 0.9129554655870445}

python BayesNetInference.py RejectionSampling ..\config\config-sprinkler.txt "P(R|C=true)" 1000
P(R)={'true': 0.7789046653144016, 'false': 0.2210953346855984}

python BayesNetInference.py RejectionSampling ..\config\config-sprinkler.txt "P(W|S=false,R=true)" 1000
P(W)={'true': 0.9090909090909091, 'false': 0.09090909090909091}


TASK 2b.
SRINKLER network								
Run	N=100		N=1000		N=10000		N=100000	
	true	false	true	false	true	false	true	false
1	NA	NA	0.9036	0.0964	0.8952	0.1048	0.8992	0.1008
2	NA	NA	0.8922	0.1078	0.8960	0.1040	0.8999	0.1001
3	NA	NA	0.8854	0.1146	0.8889	0.1111	0.8996	0.1004
4	NA	NA	0.9138	0.0862	0.8964	0.1036	0.9003	0.0997
5	NA	NA	0.9084	0.0916	0.9003	0.0997	0.9006	0.0994
AVG	NA	NA	0.9007	0.0993	0.8954	0.1046	0.8999	0.1001
STD	NA	NA	0.0117	0.0117	0.0041	0.0041	0.0005	0.0005

It can be noted that the standard deviations (STDs) of the Sprinkler network are smaller than
the Alarm network. This suggests that the number of samples should be optimised for each network.


TASK 2c.
Similar results between exact and approximate inference can be obtained. Whilst 1000 samples seem to be
sufficient for the Sprinkler network (see exact inference results below), a larger number of samples
(of at least 10K) must be considered for the Alarm network.

python BayesNetInference.py InferenceByEnumeration ..\config\config-sprinkler.txt "P(W|S=false,R=true)" 1000
normalised P(W)={'true': 0.9, 'false': 0.10000000000000002}



TASK 3.
Up to you.