1) [4pt] Unzip stock_data.zip, which contains historical market data for stocks in the S&P 500 index. The S&P 500 is a stock market index tracking the performance of 500 large companies listed on stock exchanges in the United States.
Load the data into a pandas dataframe. It should have 7 columns:
1. Date: the date for a given set of observations
2. Open: the price of a given stock on a given date, at opening time on the relevant stock exchange
3. High: the highest price reached by a given stock on a given date
4. Low: the lowest price reached by a given stock on a given date
5. Close: the price of a given stock on a given date, at closing time on the relevant stock exchange
6. Volume: the total volume traded for a given stock on a given date
7. Name: The stock name
This assignment will use the date, close and name columns.
Show the code you have used to load the data into a pandas dataframe. (It should be about 1 line.)
2) [6pt] Identify the set of all names in the data, and sort these names in alphabetical order. How many are there? List the first and last 5 names.
3) [10pt] Remove all names for which the first date is after 1st July 2014 or the last date is before 30th June 2017. Which names were removed? How many are left?
Hint: After filtering out the above names, you should be left with all of the names which have market data for at least the years period 1 July 2014 - 30 June 2017. You should have removed a small number of names: these are stocks which started trading after 1st July 2014 or ceased trading before 30th June 2017. You should be left with close to 500 names. Use these 500-ish names for the rest of the assignment. The names you removed will not be used again, because they don't have sufficient market data.
4) [10pt] Identify the set of dates that are common to all the remaining names. Remove all the dates that are before 1st July 2014 or after 30th June 2017. How many dates are left? What are the first and last 5 dates?
Hint: There are approximately 250 trading days in a year, and the period 1st July 2014 to 30th June 2017 spans 3 years. So, you should be left with approximately 250 * 3 = 750 dates. The first 5 dates should be close to (but not before) 1st July 2014. The last 5 dates should be close to (but not after) 30th June 2017.
5) [10pt] Build a new pandas dataframe which has a column for each of the names from step (3) and a row for each of the dates from step (4). The dataframe should contain the "close" values for each corresponding name and date. Call the python "print" function for your dataframe and show the result.
6) [10pt] Create another dataframe containing returns calculated as:
return(name, date) = (close(name, date) - close(name, previous date)) / close(name, previous date)
Note that this dataframe should have one less row than the dataframe from step (5), because you can't calculate returns for the first date (there's no previous date). Call the python "print" function for your dataframe and show the result.

7) [30pt] Use the class sklearn.decomposition.PCA to calculate the principal components of the returns from step (6). Print the top five PCs when ranked according to their eigenvalue (the bigger the better).
8) [40pt] For the principal components calculated in step (7), extract the explained variance ratios (you can get these from the PCA object). What percentage of variance is explained by the first principal component? Plot the first 20 explained variance ratios. Identify an elbow and mark it on the plot. Provide at least 1 paragraph description of your code.

9) [40pt] Calculate the cumulative variance ratios using numpy.cumsum on the list of explained variance ratios from step (8). Plot all these cumulative variance ratios (x axis = principal component, y axis = cumulative variance ratio). Mark on your plot the principal component for which the cumulative variance ratio is greater than or equal to 95%.
10) [40pt] Normalise your dataframe from step (6) so that the columns have zero mean and unit variance. Repeat steps (7) - (9) for this new dataframe.